"use strict";(self.webpackChunkaruku=self.webpackChunkaruku||[]).push([[533],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return h}});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),u=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=u(e.components);return r.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=u(n),h=i,g=c["".concat(l,".").concat(h)]||c[h]||d[h]||o;return n?r.createElement(g,a(a({ref:t},p),{},{components:n})):r.createElement(g,a({ref:t},p))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,a[1]=s;for(var u=2;u<o;u++)a[u]=n[u];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},7890:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return s},metadata:function(){return u},toc:function(){return d}});var r=n(7462),i=n(3366),o=(n(7294),n(3905)),a=["components"],s={id:"engine",title:"Walk Engine"},l=void 0,u={unversionedId:"engine",id:"engine",title:"Walk Engine",description:"Custom Graph Partition",source:"@site/../modules/aruku-docs/target/mdoc/engine.md",sourceDirName:".",slug:"/engine",permalink:"/aruku/docs/engine",draft:!1,editUrl:"https://github.com/pierrenodet/aruku/edit/master/../modules/aruku-docs/target/mdoc/engine.md",tags:[],version:"current",frontMatter:{id:"engine",title:"Walk Engine"},sidebar:"someSidebar",previous:{title:"Overview",permalink:"/aruku/docs/overview"},next:{title:"Configuration",permalink:"/aruku/docs/configuration"}},p={},d=[{value:"Custom Graph Partition",id:"custom-graph-partition",level:2},{value:"Routing Table",id:"routing-table",level:2},{value:"Optimized Edge Sampling",id:"optimized-edge-sampling",level:2}],c={toc:d};function h(e){var t=e.components,n=(0,i.Z)(e,a);return(0,o.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"custom-graph-partition"},"Custom Graph Partition"),(0,o.kt)("p",null,"One of the bigeast caveats of random walks in distributed environment is the ammount of data exchanged between every steps. Indeed to send a walker to his next vertice, you need to send both the walker (quite small) and a message for second order random walks."),(0,o.kt)("p",null,"In spark, message passing is costly as it means shuffling of the data. In order to reduce the shuffling, but keep a high level of parallelism, we need to work not at the partition level but at the executor level."),(0,o.kt)("p",null,"So inspired by the talk of Min Shen at Spark Summit 2017, a custom graph partition is implemented, being an adjancy list shared at the executor level. When sending a worker to his next step, we only need to shuffle the data if the next vertice is not in the same executor instead of being in a different partition. Thus reducing the amount of shuffled data."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"aruku is distributed, but not out of core")),(0,o.kt)("h2",{id:"routing-table"},"Routing Table"),(0,o.kt)("p",null,"With this custom graph partition we have actually lost a lot of cool features provided by GraphX to store distributed graph, especially fault tolerance and the automatic distributed message passing."),(0,o.kt)("p",null,"In order to get the automatic distributed message passing, as we store the graph in our custom parititons, we are going to use a predefined partitioner. As the RDD is keyed with the vertex id, if we want to send the walker to the right parition containing his next vertex, we just need to keyed the RDD of walkers with the next vertex id and repartition them by our predefined partitioner."),(0,o.kt)("p",null,"To get the fault tolerance back, instead of just repartitioning our RDD of walkers keyed by the vertex id they need to be send to, we are going to zipPartition this RDD to our routing table which is the empty RDD that is the result of storing our input RDD into our custom graph partitions. If we lose an executor we will need to recompute this RDD and fill the custom graph partition in the new executor spawned by SparK."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"aruku is fault tolerant")),(0,o.kt)("h2",{id:"optimized-edge-sampling"},"Optimized Edge Sampling"),(0,o.kt)("p",null,"Another difficult part of implementing efficient higher order random walks for big graphs is having an efficient edge sampling. Indeed if the alias sampling works well to precompute sampling for the transition probabilities, it's only suited for static walks. For dynamic walks the number of possiblities is too huge to precompute thus having clever algorithm is required."),(0,o.kt)("p",null,"KnightKing [",(0,o.kt)("a",{parentName:"p",href:"https://github.com/KnightKingWalk/KnightKing"},"github"),"] is a general-purpose, distributed graph random walk engine that proposed a number of optimizations to random walks. One of them is an optimized rejection sampling in order to do edge sampling efficiently."),(0,o.kt)("p",null,"They decomposed the transition probability in a static and dynamic componenent. For the dynamic sampling they compare it to throwing a dart in a target, with bars of width of the static component and height of the dynamic one. The static componenent is dealt with alias sampling. The dynamic is using rejection sampling and clever optimizations with lower bound bypass and clever folding."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"aruku is fast, but probably slower than KnightKing")))}h.isMDXComponent=!0}}]);