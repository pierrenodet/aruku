"use strict";(self.webpackChunkaruku=self.webpackChunkaruku||[]).push([[533],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return g}});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=r.createContext({}),u=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=u(e.components);return r.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=u(n),g=i,h=d["".concat(s,".").concat(g)]||d[g]||c[g]||a;return n?r.createElement(h,o(o({ref:t},p),{},{components:n})):r.createElement(h,o({ref:t},p))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,o=new Array(a);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var u=2;u<a;u++)o[u]=n[u];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7890:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return s},default:function(){return g},frontMatter:function(){return l},metadata:function(){return u},toc:function(){return c}});var r=n(7462),i=n(3366),a=(n(7294),n(3905)),o=["components"],l={id:"engine",title:"Walk Engine"},s=void 0,u={unversionedId:"engine",id:"engine",title:"Walk Engine",description:"Custom Graph Partition",source:"@site/../modules/aruku-docs/target/mdoc/engine.md",sourceDirName:".",slug:"/engine",permalink:"/aruku/docs/engine",draft:!1,editUrl:"https://github.com/pierrenodet/aruku/edit/master/../modules/aruku-docs/target/mdoc/engine.md",tags:[],version:"current",frontMatter:{id:"engine",title:"Walk Engine"},sidebar:"someSidebar",previous:{title:"Overview",permalink:"/aruku/docs/overview"},next:{title:"Configuration",permalink:"/aruku/docs/configuration"}},p={},c=[{value:"Custom Graph Partition",id:"custom-graph-partition",level:2},{value:"Routing Table",id:"routing-table",level:2},{value:"Optimized Edge Sampling",id:"optimized-edge-sampling",level:2}],d={toc:c};function g(e){var t=e.components,n=(0,i.Z)(e,o);return(0,a.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"custom-graph-partition"},"Custom Graph Partition"),(0,a.kt)("p",null,"One of the most significant caveats of random walks in a distributed environment is the amount of data exchanged between nodes at every step. Indeed, to send a walker to his next vertice, you need to send both the walker (relatively small) and a message for second-order random walks."),(0,a.kt)("p",null,"In spark, message passing is costly as it's equivalent to data shuffling. To reduce the overall shuffling but still keep a high level of parallelism, we need to work at the executor level instead of the partition level."),(0,a.kt)("p",null,"So, inspired by the talk of Min Shen at Spark Summit 2017 [",(0,a.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=lyVZNZZUdOk"},"talk"),"], a custom graph partition is implemented, being an adjacency list shared at the executor level. When sending a worker to his next step, we only need to shuffle the data if the next vertice is not in the same executor instead of being in a different partition, thus reducing the amount of shuffled data."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"aruku is distributed, but not out of core")),(0,a.kt)("h2",{id:"routing-table"},"Routing Table"),(0,a.kt)("p",null,"With this custom graph partition, we have lost many cool features provided by GraphX being : "),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"distributed graph storing,"),(0,a.kt)("li",{parentName:"ul"},"distributed message passing,"),(0,a.kt)("li",{parentName:"ul"},"fault tolerance.")),(0,a.kt)("p",null,"To get the distributed message passing, we need to repartition our walkers with the right partitioner to get them in the required executors. We first aggregate the input graph by collecting neighbor edges of each vertex of the graph resulting in a PairRDD keyed by vertex ID. This PairRDD is repartitioned with the partitioner from the original VertexRDD, and custom graph partitions are built. Then the partitioner is used to send the walkers to the suitable executors by repartitioning the walkers PairRDD keyed by the vertex ID to which they need to be sent."),(0,a.kt)("p",null,"To get the fault tolerance back, we will use a routing table instead of just repartitioning our PairRDD of walkers. When building custom graph partitions from the PairRDD of neighbor edges, we keep the resulting empty RDD. So, after repartitioning the walkers PairRDD, we use the zipPartition function of this empty RDD with the walkers PairRDD to create a RDD lineage that would break if spark executors are lost, or tasks are failed."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"aruku is fault tolerant")),(0,a.kt)("h2",{id:"optimized-edge-sampling"},"Optimized Edge Sampling"),(0,a.kt)("p",null,"Another tricky part of implementing efficient higher-order random walks for big graphs is the implementation of an efficient edge sampling algorithm. Indeed if the alias sampling works well to precompute sampling for the transition probabilities, it's only suited for static walks. For dynamic walks, the space of possible transitions is too huge to precompute, thus requiring a clever algorithm."),(0,a.kt)("p",null,"KnightKing [",(0,a.kt)("a",{parentName:"p",href:"https://github.com/KnightKingWalk/KnightKing"},"github"),"] is a general-purpose, distributed graph random walk engine that proposed several optimizations to random walk engines. One of these optimizations is an optimized rejection sampling to do edge sampling efficiently."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"aruku is fast, but probably slower than KnightKing")))}g.isMDXComponent=!0}}]);